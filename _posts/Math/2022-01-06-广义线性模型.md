# 广义线性模型

refitem:
- https://www.sagepub.com/sites/default/files/upm-binaries/21121_Chapter_15.pdf
- https://xg1990.com/blog/archives/304
- https://www.zhangzhenhu.com/glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html  非常好的材料



## 广义线性模型

#### 定义
 
（Generalized Linear Model, GLM）

1. 随机部分：Y 服从 某指数族分布。其概率密度函数的统一形式为

$$
P(y|\eta) = h(y) \text {exp}(\eta^T T(y) - A (\eta)) \tag1
$$

2. 系统部分

$$\eta \equiv \mathbf{X} \theta $$

3. 链接函数

定义为$y$的期望$\mu$与系统部分$\eta$之间的函数关系

$$
\eta = g(\mu)
$$
$$E(y)\equiv \mu = g^{-1}(\eta)$$


系统部分的$X \theta$得到$\eta$，$\eta$通过链接函数得到联合分布的期望$E(y)$即$\mu$，然后$y$服从于该联合概率分布，$\mu$是其均值参数。

随机部分决定概率分布的形式，系统部分通过链接函数得到该联合分布的期望曲线。

#### 特征

为什么要将$y$的分布定义为奇怪的指数族分布？因为，在这种定义下，可以推出如下特征

期望

$$
E(y|\eta) \equiv \mu= \frac{d}{d\eta}A(\eta)
$$

方差

$$
Var(y|\eta) \equiv \sigma =\frac {d^2} {d\eta^2} A(\eta)
$$

#### 推导过程

对于指数族分布的概率密度函数：

$$
P(y|\eta) = h(y) \text {exp}(\eta^T T(y) - A (\eta))
$$

首先，由于概率密度函数的积分为1，有：

$$
\int h(y) \text {exp}(\eta^T T(y) - A (\eta)) dy = 1
$$

对两边关于$\eta$求导：

$$
\begin{align}
\frac{\partial}{\partial \eta} \int h(y) \text {exp}(\eta^T T(y) - A (\eta)) dy &= \frac{\partial}{\partial \eta} 1 \\
\int h(y) \text {exp}(\eta^T T(y) - A (\eta)) (T(y) - \frac{\partial A(\eta)}{\partial \eta}) dy &= 0 \\
\int P(y|\eta) (T(y) - \frac{\partial A(\eta)}{\partial \eta}) dy &= 0
\end{align}
$$

整理得：

$$
\int P(y|\eta) T(y) dy = \frac{\partial A(\eta)}{\partial \eta}
$$

左边就是$E[T(y)]$，所以：

$$
E[T(y)] \equiv \mu = \frac{\partial A(\eta)}{\partial \eta} \tag 2
$$

再次对$\eta$求导：

$$
\int P(y|\eta) (T(y) - \frac{\partial A(\eta)}{\partial \eta})^2 dy = \frac{\partial^2 A(\eta)}{\partial \eta^2}
$$

左边就是$Var[T(y)]$，所以：

$$
Var[T(y)] \equiv \sigma^2 = \frac{\partial^2 A(\eta)}{\partial \eta^2} \tag3
$$

对于不同的分布，$T(y)$的具体形式不同，因此期望和方差的具体表达式也会不同。

#### 极大似然求解

广义线性模型，有如下求解过程，求对数似然

$$
\begin{align}
\ln L(\theta) &\equiv ln P(y|\eta)\\
&= \sum_{i=1} ^k (\ln h(y_i) + \eta T(y_i) - A(\eta)) \\
&= \sum_{i=1} ^k \ln h(y_i) + \sum_{i=1} ^k \eta T(y_i) - kA(\eta)
\end{align}
$$

为了找到最大似然估计，我们需要对$\theta$求导并令其等于0：

$$
\begin{align}
\frac{\partial \ln L(\theta)}{\partial \theta} &= \frac{\partial}{\partial \theta}[\sum_{i=1} ^k \eta T(y_i) - kA(\eta)] \\
&= \sum_{i=1} ^k T(y_i)\frac{\partial \eta}{\partial \theta} - k\frac{\partial A(\eta)}{\partial \eta}\frac{\partial \eta}{\partial \theta} \\
&= \sum_{i=1} ^k T(y_i)\frac{\partial \eta}{\partial \theta} - kE[T(y)]\frac{\partial \eta}{\partial \theta} \\
&= \sum_{i=1} ^k (T(y_i) - E[T(y)])\frac{\partial \eta}{\partial \theta} \stackrel{let}= 0
\end{align}
$$

由于$\eta = X\theta$，所以$\frac{\partial \eta}{\partial \theta} = X^T$，代入上式，得出

==广义线性模型通用的极大似然估计方程==：

$$
X^T(T(y) - E[T(y)]) = 0 \tag 4
$$

对于不同的分布，$T(y)$和$E[T(y)]$的具体形式不同，因此求解方法也会有所不同。


## 线性回归

[2022-01-04-最小二乘优化](Math/2022-01-04-最小二乘优化.md)

1. 对于线性回归而言，随机部分是正态分布，有

$$
Y  \sim \mathcal N(\mu,\sigma)
$$

$$
P(Y = y|\mathbf \mu) = \frac 1 {\sqrt{2 \pi} \sigma} \text{exp} (-\frac{(y - \mu)^2}{2 \sigma ^ 2})
$$
其中，$\mu$ 是期望值，$\sigma^2$ 是方差。

2. 写作广义线性模型的统一形式有

$$
\begin{align}
P(Y = y|\mathbf \mu) 
&= \frac 1 {\sqrt{2 \pi} \sigma} \text{exp} (-\frac{(y - \mu)^2}{2 \sigma ^ 2}) \\
&= \frac 1 {\sqrt{2 \pi} \sigma} \text{exp} (-\frac{y^2}{2 \sigma ^ 2} + \frac{y\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2}) \\
&= \text{exp}(\frac{\mu}{\sigma^2}y - \frac{\mu^2}{2\sigma^2} - \frac{y^2}{2\sigma^2} - \ln(\sqrt{2\pi}\sigma))
\end{align}
$$


3. 对照统一形式 $P(y|\eta) = h(y) \text {exp}(\eta^T T(y) - A (\eta))$，得到：
- $\eta = \frac{\mu}{\sigma^2}$
- $T(y) = y$
- $A(\eta) = \frac{\mu^2}{2\sigma^2} = \frac{\sigma^2\eta^2}{2}$
- $h(y) = \text{exp}(-\frac{y^2}{2\sigma^2} - \ln(\sqrt{2\pi}\sigma))$

4. 对于线性回归，我们通常假设方差 $\sigma^2$ 是已知的常数。在这种情况下，$\eta$ 和 $\mu$ 之间只相差一个常数因子 $\frac{1}{\sigma^2}$。

5. 由于链接函数 $g$ 是 $\mu$ 到 $\eta$ 的映射，而 $\eta = \frac{\mu}{\sigma^2}$，所以：
$$\eta = g(\mu) = \frac{\mu}{\sigma^2}$$

6. 但是，由于 $\sigma^2$ 是常数，这个映射本质上就是 $\mu$ 的线性变换。在广义线性模型中，我们通常将这种线性变换视为恒等函数，因为：
   - 常数因子 $\frac{1}{\sigma^2}$ 可以被吸收到参数 $\theta$ 中
   - 这种线性变换不会改变模型的基本结构
   - 在计算中，我们通常将 $\sigma^2$ 视为1，或者将其作为模型的一个参数来估计

因此，我们可以说线性回归的链接函数是恒等函数：
$$\eta = g(\mu) = \mu$$

这种设定使得线性回归成为广义线性模型中最简单的一个特例，其中系统部分直接等于期望值，不需要通过链接函数进行任何非线性变换。

这意味着系统部分直接等于期望值：

$$\mu = \mathbf{X}\theta$$

#### 极大似然求解

##### 直接使用结论

直接将广义线性模型的推导结果，将 $T(y)$ 和 $\mu$ 带入到表达式（4）中，即可得到正规方程：

$$
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\theta) = 0
$$

##### 详细推导

在高斯分布的GLM中，对数似然函数 $\ell$ 为：

$$\begin{align}
\ell(\theta) &= \ln P(Y|\mu) \\
&= \sum_{i=1}^n [-\frac{1}{2}\ln(2\pi\sigma^2) - \frac{(y_i-\mu_i)^2}{2\sigma^2}] \\
&= -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mu_i)^2
\end{align}$$

对参数θ求导并令其等于0：

$$\begin{align}
\frac{\partial \ell}{\partial \theta} &= \frac{\partial}{\partial \theta}[-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\mathbf{x}_i^T\theta)^2] \\
&= -\frac{1}{2\sigma^2}\sum_{i=1}^n 2(y_i-\mathbf{x}_i^T\theta)(-\mathbf{x}_i) \\
&= \frac{1}{\sigma^2}\sum_{i=1}^n(y_i-\mathbf{x}_i^T\theta)\mathbf{x}_i \\
&= \frac{1}{\sigma^2}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\theta)
\end{align}$$

令导数等于0，同样可以得到正规方程：

$$\mathbf{X}^T(\mathbf{y}-\mathbf{X}\theta) = 0$$

求解正规方程：

$$\begin{align}
\mathbf{X}^T\mathbf{y} - \mathbf{X}^T\mathbf{X}\theta &= 0 \\
\mathbf{X}^T\mathbf{X}\theta &= \mathbf{X}^T\mathbf{y} \\
\theta &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{align}$$

这就是线性回归的闭式解（解析解）。这个解有几个重要特点：

1. 它是最小二乘估计，因为最小化残差平方和等价于最大化对数似然函数
2. 它不需要迭代求解，可以直接计算
3. 当$\mathbf{X}^T\mathbf{X}$可逆时，解是唯一的
4. 这个解也是无偏估计，即$E[\hat{\theta}] = \theta$


## 逻辑回归

对于逻辑回归而言，随机部分是伯努力分布，有

$$
Y  \sim \mathcal B(\mu)
$$
$$
P(Y = y|\mathbf \mu) = \mu ^ y (1-\mu) ^ {(1-y)}, \ y\in\{0,1\}
$$

写作统一形式有

$$
\begin{align}
P(y|\mathbf \mu) 
&= \mu ^ y (1-\mu) ^ {(1-y)} \\
&= \text{exp}(y \ln \mu + (1-y) \ln (1-\mu)) \\
&= \text{exp}\bigg(y \ln {\frac \mu {1-\mu}} + \ln(1-\mu)\bigg)
\end{align}
$$

对照统一形式，得链接函数

$$
\eta = g(\mu) = \ln \frac {\mu}{1-\mu}
$$

故其联合分布的均值函数为

$$
E(y)\equiv \mu = g^{-1}(\eta) = \frac {e^{\eta}}{1 + e^{\eta}}
$$

这个函数即sigmod 函数，这也是使用此函数作为二分类问题的拟合函数的原因。


#### 极大似然求解

对于逻辑回归，对数似然函数为：

$$
\begin{align}
\ell(\theta) &= \sum_{i=1}^n [y_i \ln \mu_i + (1-y_i)\ln(1-\mu_i)] \\
&= \sum_{i=1}^n [y_i \ln \frac{e^{\eta_i}}{1+e^{\eta_i}} + (1-y_i)\ln \frac{1}{1+e^{\eta_i}}] \\
&= \sum_{i=1}^n [y_i\eta_i - \ln(1+e^{\eta_i})]
\end{align}
$$

对参数 $\theta$ 求导：

$$
\begin{align}
\frac{\partial \ell}{\partial \theta} &= \sum_{i=1}^n [y_i - \frac{e^{\eta_i}}{1+e^{\eta_i}}] \frac{\partial \eta_i}{\partial \theta} \\
&= \sum_{i=1}^n (y_i - \mu_i) \mathbf{x}_i
\end{align}
$$

令导数等于0，得到：

$$
\sum_{i=1}^n (y_i - \mu_i) \mathbf{x}_i = 0
$$

由于这个方程没有解析解，需要使用迭代方法（如牛顿法）求解。

## Softmax回归

对于Softmax回归，随机部分是多项分布：

$$
Y \sim \text{Multinomial}(n, \mu_1, \mu_2, ..., \mu_k)
$$

其中 $n$ 是样本数，$\mu_i$ 是第 $i$ 类的概率。

概率密度函数为：

$$
P(Y_1=y_1, ..., Y_k=y_k) = \frac{n!}{y_1!...y_k!}\mu_1^{y_1}...\mu_k^{y_k}
$$

写作指数族形式：

$$
\begin{align}
P(Y_1=y_1, ..., Y_k=y_k) 
&= \text{exp}(\sum_{i=1}^k y_i \ln \mu_i + \ln \frac{n!}{y_1!...y_k!}) \\
&= \text{exp}(\sum_{i=1}^{k-1} y_i \ln \frac{\mu_i}{\mu_k} + n\ln \mu_k + \ln \frac{n!}{y_1!...y_k!})
\end{align}
$$

链接函数为：

$$
\eta_i = \ln \frac{\mu_i}{\mu_k}, \quad i=1,...,k-1
$$

对应的逆链接函数为：

$$
\mu_i = \frac{e^{\eta_i}}{1+\sum_{j=1}^{k-1}e^{\eta_j}}, \quad i=1,...,k-1
$$

$$
\mu_k = \frac{1}{1+\sum_{j=1}^{k-1}e^{\eta_j}}
$$

#### 极大似然求解

Softmax回归的对数似然函数为：

$$
\ell(\theta) = \sum_{i=1}^n \sum_{j=1}^k y_{ij} \ln \mu_{ij}
$$

其中 $y_{ij}$ 是第 $i$ 个样本属于第 $j$ 类的指示变量，$\mu_{ij}$ 是模型预测的第 $i$ 个样本属于第 $j$ 类的概率。

同样需要使用迭代方法求解。

## 泊松回归

对于泊松回归，随机部分是泊松分布：

$$
Y \sim \text{Poisson}(\mu)
$$

概率密度函数为：

$$
P(Y = y|\mu) = \frac{\mu^y e^{-\mu}}{y!}
$$

写作指数族形式：

$$
P(Y = y|\mu) = \text{exp}(y\ln\mu - \mu - \ln(y!))
$$

链接函数为：

$$
\eta = g(\mu) = \ln \mu
$$

对应的逆链接函数为：

$$
\mu = g^{-1}(\eta) = e^\eta
$$

#### 极大似然求解

泊松回归的对数似然函数为：

$$
\ell(\theta) = \sum_{i=1}^n (y_i\ln\mu_i - \mu_i - \ln(y_i!))
$$

对参数 $\theta$ 求导：

$$
\frac{\partial \ell}{\partial \theta} = \sum_{i=1}^n (y_i - \mu_i)\mathbf{x}_i
$$

同样需要使用迭代方法求解。

## 链接函数一览

![](Math/glms.png)

